{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":957},"executionInfo":{"elapsed":6688,"status":"ok","timestamp":1718698759398,"user":{"displayName":"Rosencor Rosencor","userId":"03851350926450566129"},"user_tz":-120},"id":"AhbFCiyyjPjc","outputId":"aef862a3-4d4f-4ff8-98fe-5b17db5cc128"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dictionary size (words): len(y[i]): 3115\n","Total X variables: 181924\n","\n","Text train example1\n"]},{"output_type":"display_data","data":{"text/plain":["'20009kt 9999 prec0n CL5 CM0 10 07 q1026 17005KT 140V210 9999 FEW020 BKN028 09/06 Q1026 NOSIG'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Sequences example 1\n"]},{"output_type":"display_data","data":{"text/plain":["[250, 1, 2, 33, 3, 8, 16, 52, 431, 818, 1, 83, 252, 10, 18, 52, 4]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Text train example2\n"]},{"output_type":"display_data","data":{"text/plain":["'12007kt 9999 prec0n CL0 CM0 11 08 q1023 14008KT CAVOK 12/09 Q1023 NOSIG'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Sequences example 2\n"]},{"output_type":"display_data","data":{"text/plain":["[520, 1, 2, 5, 3, 6, 13, 37, 872, 15, 7, 10, 37, 4]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","X\n"]},{"output_type":"display_data","data":{"text/plain":["array([[  0,   0,   0,   0,   0, 250,   1,   2,  33,   3,   8,  16,  52],\n","       [  0,   0,   0,   0, 250,   1,   2,  33,   3,   8,  16,  52, 431],\n","       [  0,   0,   0, 250,   1,   2,  33,   3,   8,  16,  52, 431, 818],\n","       [  0,   0, 250,   1,   2,  33,   3,   8,  16,  52, 431, 818,   1],\n","       [  0, 250,   1,   2,  33,   3,   8,  16,  52, 431, 818,   1,  83],\n","       [250,   1,   2,  33,   3,   8,  16,  52, 431, 818,   1,  83, 252],\n","       [  1,   2,  33,   3,   8,  16,  52, 431, 818,   1,  83, 252,  10],\n","       [  2,  33,   3,   8,  16,  52, 431, 818,   1,  83, 252,  10,  18],\n","       [ 33,   3,   8,  16,  52, 431, 818,   1,  83, 252,  10,  18,  52],\n","       [  0,   0,   0,   0,   0, 520,   1,   2,   5,   3,   6,  13,  37],\n","       [  0,   0,   0,   0, 520,   1,   2,   5,   3,   6,  13,  37, 872],\n","       [  0,   0,   0, 520,   1,   2,   5,   3,   6,  13,  37, 872,  15],\n","       [  0,   0, 520,   1,   2,   5,   3,   6,  13,  37, 872,  15,   7],\n","       [  0, 520,   1,   2,   5,   3,   6,  13,  37, 872,  15,   7,  10],\n","       [520,   1,   2,   5,   3,   6,  13,  37, 872,  15,   7,  10,  37],\n","       [  0,   0,   0,   0,   0, 256,  36,   2,  58,   3,   6,   6,  67],\n","       [  0,   0,   0,   0, 256,  36,   2,  58,   3,   6,   6,  67, 413],\n","       [  0,   0,   0, 256,  36,   2,  58,   3,   6,   6,  67, 413, 460],\n","       [  0,   0, 256,  36,   2,  58,   3,   6,   6,  67, 413, 460,   1],\n","       [  0, 256,  36,   2,  58,   3,   6,   6,  67, 413, 460,   1, 168],\n","       [256,  36,   2,  58,   3,   6,   6,  67, 413, 460,   1, 168,   7],\n","       [ 36,   2,  58,   3,   6,   6,  67, 413, 460,   1, 168,   7,   7],\n","       [  2,  58,   3,   6,   6,  67, 413, 460,   1, 168,   7,   7,  67],\n","       [ 58,   3,   6,   6,  67, 413, 460,   1, 168,   7,   7,  67,  11],\n","       [  3,   6,   6,  67, 413, 460,   1, 168,   7,   7,  67,  11,  21]],\n","      dtype=int32)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Dictionary first 10 words\n"]},{"output_type":"display_data","data":{"text/plain":["{'9999': 1,\n"," 'prec0n': 2,\n"," 'cm0': 3,\n"," 'nosig': 4,\n"," 'cl0': 5,\n"," '11': 6,\n"," '12': 7,\n"," '10': 8,\n"," '13': 9,\n"," '09': 10}"]},"metadata":{}}],"source":["#@title Get text train and test ,X and Y\n","nrows_train = 20000 # @param {type:\"integer\"}\n","sequence_length = 13 # @param {type:\"integer\"}\n","\n","feed_lenght = 8\n","\n","import pandas as pd\n","import numpy as np\n","import time\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.layers import LSTM, Dense, Embedding\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.optimizers import Adam\n","import json\n","from keras.preprocessing.text import tokenizer_from_json\n","\n","#load fusion\n","fus = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/gpt/LEST/LESTfusion1.csv\",\n","                  parse_dates=[\"time\"], index_col=\"time\")\n","\n","#Get the train and test train\n","texts_train = fus[\"fusion\"].sample(nrows_train,)\n","texts_test = fus[\"fusion\"].drop(texts_train.index)\n","\n","#save texts test\n","texts_test.to_csv(\"/content/drive/MyDrive/Colab Notebooks/gpt/LEST/LESTtexts_test1.csv\")\n","\n","# Tokenize text\n","tokenizer = Tokenizer()\n","\n","#tokenizer.fit_on_texts(texts_train)\n","tokenizer.fit_on_texts(fus[\"fusion\"])\n","\n","#Save tokenizer\n","tokenizer_json = tokenizer.to_json()\n","\n","# Save the JSON configuration to a file\n","with open('/content/drive/MyDrive/Colab Notebooks/gpt/LEST/LESTtokenizer1.json', 'w', encoding='utf-8') as f:\n","    f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n","\n","sequences = tokenizer.texts_to_sequences(texts_train)\n","\n","# Prepare input and output data\n","X = []\n","y = []\n","for sequence in sequences:\n","    for i in range(1, len(sequence)):\n","        x_seq = sequence[:i]\n","        x_seq_padded = pad_sequences([x_seq], maxlen=sequence_length, padding='pre')\n","        X.append(x_seq_padded[0])\n","        y.append(sequence[i])\n","\n","X = np.array(X)\n","y = np.array(y)\n","\n","#filter seed/model words\n","df = pd.DataFrame(X)\n","df[\"y\"] = y\n","df_fil =  df[(df.iloc[:, :sequence_length-feed_lenght+1] != 0).any(axis=1)]\n","X = df_fil.iloc[:, :-1].values\n","y = df_fil.iloc[:, -1].values\n","\n","# One hot encode the outputs\n","y = np.eye(len(tokenizer.word_index) + 1)[y]\n","\n","print(\"Dictionary size (words): len(y[i]):\",len(y[1]) )\n","print(\"Total X variables:\",len(X) )\n","\n","print(\"\\nText train example1\")\n","display(texts_train[0])\n","\n","print(\"\\nSequences example 1\")\n","display(sequences[0])\n","\n","print(\"\\nText train example2\")\n","display(texts_train[1])\n","\n","print(\"\\nSequences example 2\")\n","display(sequences[1])\n","\n","print(\"\\nX\")\n","display(X[:25])\n","\n","print(\"\\nDictionary first 10 words\")\n","display({k: tokenizer.word_index[k] for k in list(tokenizer.word_index)[:10]})\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4fOX-9Am0V_d","executionInfo":{"status":"ok","timestamp":1718700042422,"user_tz":-120,"elapsed":1246831,"user":{"displayName":"Rosencor Rosencor","userId":"03851350926450566129"}},"outputId":"bfd4f2e7-30e4-4d14-c859-e199ea6a5438"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","356/356 [==============================] - 29s 70ms/step - loss: 3.9472 - accuracy: 0.2076\n","Epoch 2/50\n","356/356 [==============================] - 25s 69ms/step - loss: 2.8421 - accuracy: 0.2949\n","Epoch 3/50\n","356/356 [==============================] - 24s 68ms/step - loss: 2.5798 - accuracy: 0.3248\n","Epoch 4/50\n","356/356 [==============================] - 25s 69ms/step - loss: 2.4432 - accuracy: 0.3412\n","Epoch 5/50\n","356/356 [==============================] - 25s 69ms/step - loss: 2.3509 - accuracy: 0.3549\n","Epoch 6/50\n","356/356 [==============================] - 24s 68ms/step - loss: 2.2841 - accuracy: 0.3650\n","Epoch 7/50\n","356/356 [==============================] - 24s 69ms/step - loss: 2.2296 - accuracy: 0.3745\n","Epoch 8/50\n","356/356 [==============================] - 24s 69ms/step - loss: 2.1801 - accuracy: 0.3827\n","Epoch 9/50\n","356/356 [==============================] - 25s 69ms/step - loss: 2.1361 - accuracy: 0.3903\n","Epoch 10/50\n","356/356 [==============================] - 24s 68ms/step - loss: 2.0931 - accuracy: 0.3985\n","Epoch 11/50\n","356/356 [==============================] - 24s 68ms/step - loss: 2.0468 - accuracy: 0.4094\n","Epoch 12/50\n","356/356 [==============================] - 25s 69ms/step - loss: 2.0032 - accuracy: 0.4183\n","Epoch 13/50\n","356/356 [==============================] - 24s 68ms/step - loss: 1.9609 - accuracy: 0.4252\n","Epoch 14/50\n","356/356 [==============================] - 25s 69ms/step - loss: 1.9228 - accuracy: 0.4352\n","Epoch 15/50\n","356/356 [==============================] - 25s 69ms/step - loss: 1.8865 - accuracy: 0.4437\n","Epoch 16/50\n","356/356 [==============================] - 24s 69ms/step - loss: 1.8520 - accuracy: 0.4524\n","Epoch 17/50\n","356/356 [==============================] - 24s 68ms/step - loss: 1.8209 - accuracy: 0.4591\n","Epoch 18/50\n","356/356 [==============================] - 24s 68ms/step - loss: 1.7916 - accuracy: 0.4661\n","Epoch 19/50\n","356/356 [==============================] - 24s 69ms/step - loss: 1.7594 - accuracy: 0.4748\n","Epoch 20/50\n","356/356 [==============================] - 24s 69ms/step - loss: 1.7752 - accuracy: 0.4692\n","Epoch 21/50\n","356/356 [==============================] - 24s 68ms/step - loss: 1.7245 - accuracy: 0.4823\n","Epoch 22/50\n","356/356 [==============================] - 24s 69ms/step - loss: 1.6920 - accuracy: 0.4913\n","Epoch 23/50\n","356/356 [==============================] - 24s 69ms/step - loss: 1.7116 - accuracy: 0.4881\n","Epoch 24/50\n","356/356 [==============================] - 25s 69ms/step - loss: 1.7577 - accuracy: 0.4732\n","Epoch 25/50\n","356/356 [==============================] - 24s 68ms/step - loss: 1.6535 - accuracy: 0.5006\n","Epoch 26/50\n","356/356 [==============================] - 24s 69ms/step - loss: 1.6114 - accuracy: 0.5115\n","Epoch 27/50\n","356/356 [==============================] - 25s 70ms/step - loss: 1.5784 - accuracy: 0.5195\n","Epoch 28/50\n","356/356 [==============================] - 24s 69ms/step - loss: 1.5642 - accuracy: 0.5242\n","Epoch 29/50\n","356/356 [==============================] - 24s 68ms/step - loss: 1.5394 - accuracy: 0.5304\n","Epoch 30/50\n","356/356 [==============================] - 24s 68ms/step - loss: 1.5322 - accuracy: 0.5325\n","Epoch 31/50\n","356/356 [==============================] - 25s 69ms/step - loss: 1.5245 - accuracy: 0.5345\n","Epoch 32/50\n","356/356 [==============================] - 24s 68ms/step - loss: 1.5093 - accuracy: 0.5387\n","Epoch 33/50\n","356/356 [==============================] - 24s 68ms/step - loss: 1.4984 - accuracy: 0.5415\n","Epoch 34/50\n","356/356 [==============================] - 24s 69ms/step - loss: 1.4896 - accuracy: 0.5443\n","Epoch 35/50\n","356/356 [==============================] - 25s 69ms/step - loss: 1.4881 - accuracy: 0.5420\n","Epoch 36/50\n","356/356 [==============================] - 24s 68ms/step - loss: 1.4822 - accuracy: 0.5435\n","Epoch 37/50\n","356/356 [==============================] - 24s 68ms/step - loss: 1.4582 - accuracy: 0.5505\n","Epoch 38/50\n","356/356 [==============================] - 25s 69ms/step - loss: 1.4364 - accuracy: 0.5581\n","Epoch 39/50\n","356/356 [==============================] - 25s 70ms/step - loss: 1.4401 - accuracy: 0.5549\n","Epoch 40/50\n","356/356 [==============================] - 24s 68ms/step - loss: 1.4617 - accuracy: 0.5491\n","Epoch 41/50\n","356/356 [==============================] - 24s 68ms/step - loss: 1.4401 - accuracy: 0.5531\n","Epoch 42/50\n","356/356 [==============================] - 24s 69ms/step - loss: 1.4156 - accuracy: 0.5597\n","Epoch 43/50\n","356/356 [==============================] - 24s 68ms/step - loss: 1.4991 - accuracy: 0.5385\n","Epoch 44/50\n","356/356 [==============================] - 24s 68ms/step - loss: 1.6196 - accuracy: 0.5094\n","Epoch 45/50\n","356/356 [==============================] - 24s 69ms/step - loss: 1.8280 - accuracy: 0.4677\n","Epoch 46/50\n","356/356 [==============================] - 24s 69ms/step - loss: 2.0096 - accuracy: 0.4259\n","Epoch 47/50\n","356/356 [==============================] - 24s 69ms/step - loss: 1.9377 - accuracy: 0.4340\n","Epoch 48/50\n","356/356 [==============================] - 24s 68ms/step - loss: 1.8499 - accuracy: 0.4507\n","Epoch 49/50\n","356/356 [==============================] - 24s 68ms/step - loss: 1.8866 - accuracy: 0.4413\n","Epoch 50/50\n","356/356 [==============================] - 25s 69ms/step - loss: 1.7793 - accuracy: 0.4618\n"]}],"source":["#@title Train the model\n","\n","from keras.layers import Bidirectional\n","\n","# Build the LSTM model\n","model = Sequential([\n","    Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=20, input_length=sequence_length),\n","    Bidirectional(LSTM(130)),\n","\n","    Dense(len(tokenizer.word_index)+1, activation='softmax')\n","])\n","\n","\n","# Compile the model\n","model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.01), metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(X, y, epochs=50,batch_size=512)\n","\n","#save the model\n","model.save(\"/content/drive/MyDrive/Colab Notebooks/gpt/LEST/LESTmodel1.keras\")\n"]}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","provenance":[{"file_id":"1PKiCntrB2pBLVWZRaL6x-uJpEvPlNmAg","timestamp":1717661221746},{"file_id":"1qsxpGdTrTbO4obEJ4QwGeDJUpuX7gXoW","timestamp":1717492644174},{"file_id":"1LGw-aME2JOnpzAyMOVamYWUAywgSV5mI","timestamp":1715852482288},{"file_id":"1_9maObId68xlnKd6JjYqJUMRMI14ASnx","timestamp":1715845357828},{"file_id":"1BterrhINI5z4G_D4Ntuk7p8a0iuZu7ql","timestamp":1714808806688},{"file_id":"1sqdm_O_jJnvjiOLxkCsuGODjv7CDiB0l","timestamp":1714549345450},{"file_id":"1GD83k4KMSFWH42Th2LjdPwK-hN39h8RT","timestamp":1713425984380}],"mount_file_id":"1AvkgparHNZVVJ8FXg3B66k3ZS1i0-vp8","authorship_tag":"ABX9TyMh7DR9pAP4A39B9h52Fhez"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}